<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2023-02-22T15:09:56+03:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Connecting-the-Dots</title><subtitle>All about providing some examples to the topics that seems to be boring, but very interesting to demystify!</subtitle><entry><title type="html">Angular Correlations and The First Insight Behind X17 Existence</title><link href="http://localhost:4000/jekyll/update/2023/02/21/angular_correlation.html" rel="alternate" type="text/html" title="Angular Correlations and The First Insight Behind X17 Existence" /><published>2023-02-21T17:29:00+03:00</published><updated>2023-02-21T17:29:00+03:00</updated><id>http://localhost:4000/jekyll/update/2023/02/21/angular_correlation</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2023/02/21/angular_correlation.html"><![CDATA[<p>In this post Lorentz transformations, effects of boosting on opening angles, and how the former two can help making a new discovery will be covered.<br />
First things first.</p>

<h3 id="lorentz-transformations">Lorentz Transformations</h3>

<p>Lorentz transformations are the special relativistic transformations that account for rotations and relativistic boosts. It describes how a constant motion in one reference frame is seen from another reference frame including the special relativistic effects.</p>

<div style="text-align: center"><img src="/assets/images/pair_angular_correlation/frames.png" /></div>

<p>The motion in the inertial frame of reference $S$ can be described in another inertial reference frame $S’$ as below:</p>

\[\begin{equation}
(p')^\mu = \Lambda^\mu_\nu p^\nu,
\end{equation}\]

<p>where $p’$ is the 4-momentum in the frame $S$, $p$ is the 4-momentum in the frame $S$, and $\Lambda$ is the Lorentz transformation matrix. If we assume that $S’$ moves in the $\hat{x}$ direction with a relativistic speed , then we can write the transformation, and inverse transformation matrices as:</p>

\[\begin{align}
\Lambda &amp;= 

\begin{pmatrix}
    \gamma &amp; -\gamma \beta &amp;  &amp;  \\
    -\gamma \beta &amp; \gamma &amp;  &amp;  \\
     &amp;  &amp;  1 &amp;  \\
     &amp; &amp; &amp; 1 \\
\end{pmatrix} \\ \nonumber \\


\Lambda^{-1} &amp;= 

\begin{pmatrix}
    \gamma &amp; +\gamma \beta &amp;  &amp;  \\
    +\gamma \beta &amp; \gamma &amp;  &amp;  \\
     &amp;  &amp;  1 &amp;  \\
     &amp; &amp; &amp; 1 \\
\end{pmatrix} \\
\end{align}\]

<p>An object that is not moving in the $S’$ frame has the following 4-momentum in $S$ frame:</p>

\[\begin{equation}
    p = \Lambda^{-1} p' = 
    \begin{pmatrix}
        \gamma &amp; +\gamma \beta &amp;  &amp;  \\
        +\gamma \beta &amp; \gamma &amp;  &amp;  \\
        &amp;  &amp;  1 &amp;  \\
        &amp; &amp; &amp; 1 \\
    \end{pmatrix} 
    \begin{pmatrix}
        E' \\
        0 \\
        0 \\
        0 \\
    \end{pmatrix} = 
    \begin{pmatrix}
        \gamma E'  \\
        \beta \gamma E' \\
        0 \\
        0 \\
    \end{pmatrix}
\end{equation}\]

<p>As expected it is moving in the direction $\hat{x}$ of $S$ frame. Using this simple transformation one can obtain how a 2-body decay is seen in the $S$ frame. Just like every phenomena in physics nuclear decays should conserve the momentum and energy. Therefore, if a particle decays at rest into two particles with the same mass, say an electron and a positron, these should have the same energy, and the same momentum, but pointing in the opposite direction. Now, if we put the unstable particle in the reference frame $S’$ motionless, we can describe how the observer at $S$ sees its decay. Before we move on let’s give $S$, and $S’$ aliasses which are widely used by the physics communities LAB frame, and the Center of mass (CM) frame. The latter is called center of mass frame as the body decays the only thing that remains stationary will be the center of mass of the particles. Starting with the 4-momenta in the CM frame:</p>

\[\begin{equation}
    p'_{e_+} = 
    \begin{pmatrix}
        E'  \\
        p_x' \\
        p_y' \\
        0 \\
    \end{pmatrix}, \quad

    p'_{e_-} = 
    \begin{pmatrix}
        E'  \\
        -p_x' \\
        -p_y' \\
        0 \\
    \end{pmatrix}
\end{equation}\]

<p>Let’s see the same 4-momenta in the LAB frame:</p>

\[\begin{equation}
    p_{e_+} = 
    \begin{pmatrix}
        \gamma E' + \beta \gamma p_x'  \\
        \beta \gamma E' + \gamma p_x' \\
        p_y' \\
        0 \\
    \end{pmatrix}, \quad

    p_{e_-} = 
    \begin{pmatrix}
        \gamma E' - \beta \gamma p_x'  \\
        \beta \gamma E' - \gamma p_x' \\
        -p_y' \\
        0 \\
    \end{pmatrix}
\end{equation}\]

<p>Out of curiosity, let’s check the angle between the $e^+$ and $e^-$:</p>

\[\small
\begin{equation}
cos(\theta) = \frac{\vec{p_{e^+}} \cdot \vec{p_{e^-}}}{\lvert \lvert \vec{p_{e^+}} \rvert \rvert \ \lvert \lvert \vec{p_{e^-}} \rvert \rvert} = \frac{(\beta^2 \gamma^2 E'^2 - \gamma^2 p'^{2}_{x}) + p'^{2}_{y}}{\sqrt{\beta^2 \gamma^2 E'^2 + 2 \beta \gamma^2 E' p'_x + \gamma^2 p'^2_x + p'^2_y} \sqrt{\beta^2 \gamma^2 E'^2 - 2 \beta \gamma^2 E' p'_x + \gamma^2 p'^2_x + p'^2_y} }
\end{equation}\]

<p>Phew! Fotunately we have computers and don’t need to go through all this pain again. Let’s see some examples:</p>

<div style="text-align: center"><img src="/assets/images/pair_angular_correlation/boost1.png" /></div>
<p><br /><br /></p>
<div style="text-align: center"><img src="/assets/images/pair_angular_correlation/boost2.png" /></div>

<p>In the figures above you might notice X17 frame. For now, just think of X17 as a particle that decays at rest, therefore when I reference X17 frame I am talking about the CM frame. The second figure shows the minimum angle that is measured at the LAB frame, whereas at the CM frame we know that the opening angle always equals to $180^\circ$. There is an easier method to find the minimum opening angle. If the $e^+e^-$ pair moves on an axis perpendicular to the axis of boost, then the minimum opening angle calculation is as follows:</p>

\[\begin{equation}
    \begin{pmatrix}
        \gamma &amp; +\gamma \beta &amp;  &amp;  \\
        +\gamma \beta &amp; \gamma &amp;  &amp;  \\
        &amp;  &amp;  1 &amp;  \\
        &amp; &amp; &amp; 1 \\
    \end{pmatrix} 
    \begin{pmatrix}
        E' \\
        0 \\
        p'_y \\
        0 \\
    \end{pmatrix} = 
    \begin{pmatrix}
        E' \gamma \\
        E' \gamma \beta \\
        p'_y \\
        0 \\
    \end{pmatrix}
\end{equation}\]

\[\begin{equation}
    \theta_{\pm} = atan(\frac{p'_y}{\pm E' \gamma \beta} )
\end{equation}\]

<p>When we perform a Monte-Carlo (MC) simulation, and assume a uniform distribution for the angular distribution of the $e^+e^-$ pair at the CM frame, we see a modified distribution in the LAB frame.</p>

<div style="text-align: center"><img src="/assets/images/pair_angular_correlation/histo1.png" /></div>

<p>Say we observe them both in a decay, as in the figure below. Then what we could infer is that we might be observing a decay at rest, and another decay in a boosted frame. However, this is not always the case and this should never be generalized. Here, we only give this picture as a toy model:</p>

<div style="text-align: center"><img src="/assets/images/pair_angular_correlation/histo2.png" /></div>

<p><br /><br /></p>
<h3 id="a-more-realistic-picture-hypothetical-x17-particle">A More Realistic Picture: Hypothetical X17 Particle</h3>

<div style="text-align: center"><img src="/assets/images/pair_angular_correlation/ATOMKI.png" /></div>
<p><em>Reference:</em> <a href="https://arxiv.org/abs/1608.03591"><em>arXiv:1608.03591</em></a></p>

<p>In 2016 at ATOMKI laboratories in Hungary, scientist observed an anomaly in the $^8Be$ decay. They bombarded a $^7Li$ target with protons and measured an excess of particles scattering with large opening angles. According to the theory they were expexting only an anisotropic distribution of $e^+e^-$ particles which emerge from internal pair creation (IPC) events via the decay of excited \(^8Be^*\) atoms. They found that this excess can be explained if the \(^8Be^*\) atom first decays into a hypothetical boson ($X17$) and then the boson decays into $e^+e^-$ pairs. Since the X17 decays in a moving CM frame the angular distributions measured will accrete in the larger angles. We will not cover all the details of the experiment, but only show how such a distribution can be generated and analysed on your PC.</p>

<p>We first need to generate an anisotropic distribution to simulate the IPC events. Since I am not a nuclear physicists I can’t really tell why a nuclear decay is anisotropic, and calculate the angular distributions. What I can do is to guess an equation, generate some background IPC events, and then fit it.</p>

\[\begin{equation}
W(\theta) = A_0 + 0.56A_1cos(\theta) + 0.23A_2cos(\theta)^2,
\end{equation}\]

<p>where the parameters are $A_0=1$, $A_1=0.56$, $A_2=0.23$ could be a starting point. To generate random samples following a custom function, one needs to input a uniform random variable in the inverse cumulative distribution function. After that, each uniform random variable that your computer generates will be accurately matched to your custom distribution function. See them below:</p>

<div style="text-align: center"><img src="/assets/images/pair_angular_correlation/custom_pdf.png" /></div>

<p>These account for the IPC events. Now, adding X17 decay signals in the picture, we obtain the following histogram:</p>

<div style="text-align: center"><img src="/assets/images/pair_angular_correlation/histo3.png" /></div>

<p>Here, the red histogram shows the distribution of the anisotropic IPC distribution, and the blue one is the distribution of all the generated signals (including the X17 decays). To get these histograms, I generated 10,000 IPC events, and 400 X17 decays.</p>

<p>The next step is the background subtraction from the signal region and measure the statistical significance of the signals. A simple calssic method to do that is “the sideband subtraction”. The background is fitted in the regions where the signals is not expected to be present, and then this fitted level is subtracted from the total measurements, leaving only the signals. Then the significances of the measuremetns can be evaluated. Both of these results can be seen below:</p>

<div style="text-align: center"><img src="/assets/images/pair_angular_correlation/histo4.png" /></div>

<div style="text-align: center"><img src="/assets/images/pair_angular_correlation/histo5.png" /></div>

<p><br /><br /></p>

<p>These calculations were only a mere demonstration of the subject which might draw interest of some people, and perhaps help them make great discoveries someday.</p>

<p><strong>N.B.</strong> You can find the code that is used to generate the plots <a href="https://www.github.com/alikaanguven/connecting-the-dots.github.io/tree/main/assets/notebooks/pair_angular_correlations">here</a>.</p>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[In this post Lorentz transformations, effects of boosting on opening angles, and how the former two can help making a new discovery will be covered. First things first.]]></summary></entry><entry><title type="html">Mathematics of Diagonalization</title><link href="http://localhost:4000/jekyll/update/2023/02/13/mathematics-of-diagonalization.html" rel="alternate" type="text/html" title="Mathematics of Diagonalization" /><published>2023-02-13T18:52:00+03:00</published><updated>2023-02-13T18:52:00+03:00</updated><id>http://localhost:4000/jekyll/update/2023/02/13/mathematics-of-diagonalization</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2023/02/13/mathematics-of-diagonalization.html"><![CDATA[<h3 id="summary-for-tldr-community">Summary for TL;DR community</h3>

\[\begin{equation}
A = Q \Lambda Q^{-1}
\end{equation}\]

<ol>
  <li>
    <p>If an $A_{n \times n}$ matrix has $n$ distinct eigenvalues:</p>

    <p>$Q$ is the matrix where all the columns are eigenvectors of $A$, and $Q$ is invertible.<br />
 $\Lambda$ is a diagonal matrix with Complex eigenvalues of $A$ as its entries.</p>
  </li>
  <li>
    <p>If an $A_{n \times n}$ matrix is Real and symmetric:</p>

    <p>$Q$ is the matrix where all the columns are eigenvectors of $A$, and $Q$ is orthogonal.<br />
 $\Lambda$ is a diagonal matrix with Real eigenvalues of $A$ as its entries.</p>
  </li>
  <li>
    <p>If an $A_{n \times n}$ matrix is Hermitian:</p>

    <p>$Q$ is the matrix where all the columns are eigenvectors of $A$, and $Q$ is unitary.<br />
 $\Lambda$ is a diagonal matrix with Real eigenvalues of $A$ as its entries.</p>
  </li>
  <li>
    <p>If an $A_{n \times n}$ matrix is normal:</p>

    <p>$Q$ is the matrix where all the columns are eigenvectors of $A$, and $Q$ is unitary.<br />
 $\Lambda$ is a diagonal matrix with Complex eigenvalues of $A$ as its entries.<br />
<br /><br />
<br /><br /></p>
  </li>
</ol>

\[\begin{equation}
M = L M_{diag} L^{\intercal}
\end{equation}\]

<ul>
  <li>
    <p>If an $M_{n \times n}$ matrix is Complex and symmetric (Autonne-Takagi):</p>

    <p>$M$ is a diagonal matrix with non-negative eigenvalues of $M^\dagger M$ as its entries.<br />
  The columns of $L$ are eigenvectors of $M M^\dagger$.</p>
  </li>
</ul>

\[\begin{equation}
M = L M_{diag} R^{\dagger}
\end{equation}\]

<ul>
  <li>
    <p>If an $A_{m \times n}$ matrix is normal (SVD):</p>

    <p>$M$ is a diagonal matrix with non-negative eigenvalues of $A^\dagger A$ as its entries.<br />
  The columns of $L$ are eigenvectors of $M M^\dagger$.<br />
  The columns of $R$ are eigenvectors of $M^\dagger M$.</p>
  </li>
</ul>

<p><br /><br />
<br /><br /></p>

<h3 id="spectral-theorem">SPECTRAL THEOREM</h3>

<p>Spectral theorem is all about when and how a matrix can be diagonalized.</p>

<p><strong>Q:</strong> When is a matrix diagonalizable?<br />
<strong>A:</strong> Let’s start by writing the eigenvalue problem in matrix form. Let $A$ be an $(n \times n)$ matrix, and $\lambda$ and $v$ represent the corresponding eigenvalues and eigenvectors.</p>

\[\begin{align}
Av &amp;= \lambda v \\
A Q &amp;= \Lambda Q \\ \nonumber \\

A_{n \times n}

\begin{bmatrix}
    v_{11} &amp; v_{21} &amp; \dots &amp; v_{n1} \\
    v_{12} &amp; v_{22} &amp; \dots &amp; v_{n2} \\
    \vdots &amp; \vdots &amp; \ddots &amp;  \\
    v_{1n} &amp; v_{2n} &amp;  &amp; v_{nn} \\
\end{bmatrix}

&amp;=

\begin{bmatrix}
    \lambda_{1} &amp;  &amp;  &amp;  \\
        &amp; \lambda_{2} &amp;  &amp;  \\
        &amp;  &amp; \ddots &amp;  \\
        &amp;  &amp;  &amp; \lambda_{n} \\
\end{bmatrix}


\begin{bmatrix}
    v_{11} &amp; v_{21} &amp; \dots &amp; v_{n1} \\
    v_{12} &amp; v_{22} &amp; \dots &amp; v_{n2} \\
    \vdots &amp; \vdots &amp; \ddots &amp;  \\
    v_{1n} &amp; v_{2n} &amp;  &amp; v_{nn} \\
\end{bmatrix}


\end{align}\]

<p>If $Q$ matrix is constructed to have eigenvectors of A as its columns, and $\Lambda$ is constructed in a way that is diagonal which is formed by the eigenvalues of $A$, then the eigenvalue problem is succefully expressed with matrices<a href="https://math.okstate.edu/people/binegar/3013-S99/3013-l16.pdf">$^{[1]}$</a>. At this point if $Q$ is assumed to be <strong>invertible</strong>, we can further deduce:</p>

\[\begin{equation}
A = Q \Lambda Q^{-1}
\label{diagonalization}
\end{equation}\]

<p>However, we have to make sure that $Q$ is indeed invertible. If all the columns of $Q$ i.e. eigenvectors of $A$ are linearly independent, $Q$ has an inverse. Therefore $A$ must have $n$ eigenvalues associated with $n$ distinct eigenvectors <a href="https://math.berkeley.edu/~arash/54/notes/5_3.pdf">$^{[2]}$</a>. Another way to put this is to say “the algebraic multiplicities of each eigenvalue of $A$ should be equal to the geometric multiplicity”.</p>

<p>The spectral theorem provides more information on some special cases.</p>
<ul>
  <li>Cauchy formulated diagonalization for \(\mathbb{R}\)eal, and symmetric matrices.</li>
  <li>von Neumann generalized the theory to Hermitian (self-adjoint) matrices, and normal matrices.</li>
</ul>

<!-- 
Before we move on maybe it is beneficial to introduce another relation. Multiplying the right hand side of the 3<sup>rd</sup> equation by $x$.

$$
\begin{align}
Ax &= Q \Lambda Q^{-1} x \\


\end{align}
$$

 -->

<!-- Consider a linear map $A$ defined on a vector space $V$ equipped with an inner-product $\langle \cdot,\cdot \rangle$.

$$
\begin{align}
A&: \qquad V \rightarrow V \qquad V \in \mathbb{C}^{n} \\
\nonumber \\
\langle \cdot,\cdot \rangle&: V \times V \rightarrow K \quad \ \ \ K \in \mathbb{R}  \\
\end{align}
$$ -->

<p><strong>Theorem states</strong><br />
If \(A\) is Hermitian ($A^{\dagger}=A$) on some vector space $V$, then:</p>

<p><strong>1.</strong> each eigenvalue of A is \(\mathbb{R}\)eal,    <br />
<strong>2.</strong> there exists an orthonormal basis of \(V\) consists of eigenvalues of \(A\).</p>

<p><strong>Proof of 1</strong><br />
Say $x$ is an eigenvector, and $\lambda$ is the corresponding eigenvalue of $A$. Then:</p>

\[\begin{align}
Ax &amp;= \lambda x \\
\langle Ax, x \rangle &amp;= \langle x, Ax \rangle \nonumber \\
\langle \lambda^{*} x, x \rangle &amp;= \langle x, \lambda x \rangle  \nonumber \\
\lambda^{*} \langle x,x \rangle &amp;= \lambda \langle x,x \rangle  \nonumber \\
\end{align}\]

\[\begin{equation}
\lambda^{*} = \lambda
\end{equation}\]

<p><strong>Proof of 2</strong><br />
We are trying to show that if A is Hermitian, eigenvectors to different eigenvalues are perpendicular<a href="https://people.math.harvard.edu/~knill/teaching/math22b2019/handouts/lecture17.pdf">$^{[3]}$</a>.</p>

<p><strong>Assume</strong> two distinct eigenvalues ($\lambda \neq \mu$).</p>

\[\begin{align}

A v &amp;= \lambda v \\
A w &amp;= \mu w \\
\end{align}\]

<p><strong>Then:</strong></p>

\[\begin{align}

\lambda \langle v | w \rangle &amp;= \langle \lambda v | w \rangle = \lambda \langle A v | w \rangle \\
&amp;= \langle v | A w \rangle = \lambda \langle v | \mu w \rangle \\
&amp;= \mu \langle v | w \rangle \\
\end{align}\]

<p><strong>But:</strong>  $\lambda \neq \mu$,<br />
<strong>Then:</strong> $\langle v | w \rangle = 0$ should be true. <em>Q.E.D.</em></p>

<p>So far we have demonstrated that the eigenvalues of Hermitian matrices are Real, and eigenvectors for different eigenvalues are orthogonal, but these demonstrations are useful only for the diagonalization of matrices with a simple spectrum. Let’s define what simple spectrum is.</p>

<p><strong>Definition:</strong> If $A_{n \times n}$ is a matrix with all different eigenvalues, A is called to have a <strong>simple spectrum</strong>.</p>

<p>We can now proceed with proving the diagonalizability for any Hermitian matrix, but maybe it is beneficial to introduce one relation that we are going to use to prove it.</p>

<p><strong>Completeness relation:</strong> is a mathematical trick that physicists use most often to decompose a vector into its components of the space. Let’s, again, start with the eq. $\ref{diagonalization}$. Proving the eigenvectors are orthogonal for different eigenvalues of the matrix $A$, matrix $Q$ has now the ortogonal eigenvectors as its column. Remembering that orthonormal columns is a strict requirement for a unitary matrix, we understand that the matrix Q should be unitary<a href="https://math.stackexchange.com/questions/1688950/why-do-the-columns-of-a-unitary-matrix-form-an-orthonormal-basis">$^{[4]}$</a> (<strong>N.B.</strong> If Q is both Real and unitary, it is an orthogonal matrix). <strong>Thus</strong>, $Q^{-1}=Q^{\dagger}$.</p>

\[A x = Q \Lambda Q^{-1} x \\\\

\begin{align}

    
    
     &amp;= \begin{bmatrix}
        v_{11} &amp; v_{21} &amp; \dots &amp; v_{n1} \\
        v_{12} &amp; v_{22} &amp; \dots &amp; v_{n2} \\
        \vdots &amp; \vdots &amp; \ddots &amp;  \\
        v_{1n} &amp; v_{2n} &amp;  &amp; v_{nn} \\
    \end{bmatrix}
    
    \begin{bmatrix}
        \lambda_{1} &amp;  &amp;  &amp;  \\
         &amp; \lambda_{2} &amp;  &amp;  \\
         &amp;  &amp; \ddots &amp;  \\
         &amp;  &amp;  &amp; \lambda_{n} \\
    \end{bmatrix}
    
    \begin{bmatrix}
        v_{11}^{*} &amp; v_{12}^{*} &amp; \dots &amp; v_{1n}^{*} \\
        v_{21}^{*} &amp; v_{22}^{*} &amp; \dots &amp; v_{2n}^{*} \\
        \vdots &amp; \vdots &amp; \ddots &amp;  \\
        v_{n1}^{*} &amp; v_{n2}^{*} &amp;  &amp; v_{nn}^{*} \\
    \end{bmatrix}

    \begin{bmatrix}
    x_1 \\
    x_2 \\
    \vdots\\
    x_n
    \end{bmatrix} \nonumber \\ \nonumber \\


    &amp;= \begin{bmatrix}
        v_{11} &amp; v_{21} &amp; \dots &amp; v_{n1} \\
        v_{12} &amp; v_{22} &amp; \dots &amp; v_{n2} \\
        \vdots &amp; \vdots &amp; \ddots &amp;  \\
        v_{1n} &amp; v_{2n} &amp;  &amp; v_{nn} \\
    \end{bmatrix}
    
    \begin{bmatrix}
        \lambda_{1} &amp;  &amp;  &amp;  \\
         &amp; \lambda_{2} &amp;  &amp;  \\
         &amp;  &amp; \ddots &amp;  \\
         &amp;  &amp;  &amp; \lambda_{n} \\
    \end{bmatrix}
    
    \begin{bmatrix}
        v_{1}^{\dagger} x \\
        v_{2}^{\dagger} x \\
        \vdots \\
        v_{n}^{\dagger} x 
    \end{bmatrix} \nonumber \\ \nonumber \\

    &amp;= \begin{bmatrix}
        v_{11} &amp; v_{21} &amp; \dots &amp; v_{n1} \\
        v_{12} &amp; v_{22} &amp; \dots &amp; v_{n2} \\
        \vdots &amp; \vdots &amp; \ddots &amp;  \\
        v_{1n} &amp; v_{2n} &amp;  &amp; v_{nn} \\
    \end{bmatrix}
    
    \begin{bmatrix}
        \lambda_{1} v_{1}^{\dagger} x \\
        \lambda_{2} v_{2}^{\dagger} x \\
        \vdots \\
        \lambda_{n} v_{n}^{\dagger} x 
    \end{bmatrix} \nonumber \\ \nonumber \\

    &amp;= \begin{bmatrix}
        v_{11} \lambda_{1} v_{1}^{\dagger} x  + v_{21} \lambda_{2} v_{2}^{\dagger} x + \dots + v_{n1} \lambda_{n} v_{n}^{\dagger} x \\
        v_{12} \lambda_{1} v_{1}^{\dagger} x  + v_{22} \lambda_{2} v_{2}^{\dagger} x + \dots + v_{n2} \lambda_{n} v_{n}^{\dagger} x \\
        \vdots \\
        v_{1n} \lambda_{1} v_{1}^{\dagger} x  + v_{2n} \lambda_{2} v_{2}^{\dagger} x + \dots + v_{nn} \lambda_{n} v_{n}^{\dagger} x \\
    \end{bmatrix} \label{relation_harvard} \\ \nonumber \\

    &amp;= \sum_{k} v_{k} \lambda_{k} v_{k}^{\dagger} x = \sum_{k} \lambda_{k} v_{k} v_{k}^{\dagger} x \\\\
    
\end{align}\]

\[\begin{equation}
A = \sum_{k} \lambda_{k} v_{k} v_{k}^{\dagger}
  = \sum_{k} \lambda_{k} | v_{k} \rangle \langle v_{k}  |
\label{completeness}
\end{equation}\]

<p>This final equation $\ref{completeness}$ will be useful in the next proof. I anticipate there might be one single confusion with the equation $\ref{relation_harvard}$. It took me a while to understand, but we can summarize this technique as $B y = \sum_{k} b_k y_k$, where $b_k$ are the columns of $B$, and $y_k$ are the elements of the vector $y$<a href="https://math.stackexchange.com/questions/331826/expressing-a-matrix-as-an-expansion-of-its-eigenvalues">$^{[5]}$</a>. If everything is fine upto this point, we can continue with the rest of the proof.</p>

<p>Well done! We are now arriving at the “wiggle theorem”, or the “perturbation theorem”. Beware physicists! This is not the perturbation theory of physics, however you will see that it bears strong resemblance to the perturbation theory used in physics.</p>

<p><strong>Wiggle Theorem:</strong> A symmetric (Hermitian) matrix can be approximated by symmetric (Hermitian) matrices with simple spectrum <a href="https://people.math.harvard.edu/~knill/teaching/math22b2019/handouts/lecture17.pdf">$^{[3]}$</a>.</p>

<p><strong>Proof</strong></p>

<p><strong>Let</strong> $A$ be an $n \times n$ symmetric (Hermitian) matrix. It has an eigenvalue $\lambda_1$ and an eigenvector $v_1$.</p>

<p><strong>Assume</strong> $v_1$ is normalized.</p>

<p><strong>Define:</strong>
\(\begin{align}
A(t) &amp;= A + t v_1 v_1^{\dagger} \\
&amp;OR \nonumber \\
A(t) &amp;= A + t |v_1\rangle \langle v_1| 
\end{align}\)</p>

<p>By completeness relation, this $A(t)$ has the same eigenvector $v_1$ with a slighlty different eigenvalue $(\lambda_1 + t)$.
Remember that the eigenvalues of symmetric/Hermitian matrices were Real. Therefore this new $t$ should also be Real.</p>

<p>The above equations show that a curve $A(t)$ can be defined in such a way that all the eigenvectors stay constant, and the eigenvalues are slighly perturbed.</p>

<p>Still, we require that there should be $n$ distinct eigenvectors in $A$. How do we know that Hermitian matrices have this property? Check <a href="https://math.stackexchange.com/questions/227695/n-distinct-eigenvectors-for-an-n-times-n-hermitian-matrix">$^{[6]}$</a>. The proof involves Schur decomposition, and if you don’t know that see the proof of Schur decomposition here <a href="https://www.statlect.com/matrix-algebra/Schur-decomposition">$^{[7]}$</a>. This is necessary to obtain an invertible $Q$ matrix where all the columns are linearly independent. What we aim to do is to diagonalize a matrix where it does not have $n$ distinct eigenvalues. The repeated eigenvalues can be slightly perturbed to obtain a new matrix with a simple spectrum $A(t)$. However, at the end of our calculations, we need to prove that this diagonalizations still exists in the limit $t \rightarrow 0$. Let’s continue our discussion.</p>

<p>Imagine that we have defined that matrix $A(t)$ with simple spectrum. This means we finally obtained $n$ distinct eigenvalues. Remember that the matrix $Q$ was constructed from the eigenvectors of $A$, thus also $A(t)$. The matrix $A$ was Hermitian, which means the eigenvectors for different eigenvalues are also orthogonal. If they are orthogonal, I can obtain my new $Q(t)$ from orthonormal eigenvectors of $A(t)$. But hey were the eigenvalues of $A$ originally, so I will call them simly $Q$. Thus:</p>

\[A(t) = Q \Lambda(t) Q^{-1}\]

<p>Taking the limit $t \rightarrow 0$ we obtain eq. the eq. $\ref{diagonalization}$. Thus we proved all the Hermitian matrices are diagonalizable via use of unitary matrices, and all the symmetric matrices are diagonalizable by using orthogonal matrices.</p>

<!-- $$
\begin{align}

\end{align}
$$
 -->

<p><strong>Sources</strong></p>
<ul>
  <li>Steen, L. A. “Highlights in the History of Spectral Theory.” The American Mathematical Monthly 80, no. 4 (1973): 359–81. https://doi.org/10.2307/2319079.</li>
</ul>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Summary for TL;DR community]]></summary></entry><entry><title type="html">Welcome to Jekyll!</title><link href="http://localhost:4000/jekyll/update/2023/02/08/welcome-to-jekyll.html" rel="alternate" type="text/html" title="Welcome to Jekyll!" /><published>2023-02-08T16:16:27+03:00</published><updated>2023-02-08T16:16:27+03:00</updated><id>http://localhost:4000/jekyll/update/2023/02/08/welcome-to-jekyll</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2023/02/08/welcome-to-jekyll.html"><![CDATA[\[\sum_{n=1}^\infty 1/n^2 = \frac{\pi^2}{6}\]

<p>You’ll find this post in your <code class="language-plaintext highlighter-rouge">_posts</code> directory. Go ahead and edit it and re-build the site to see your changes. You can rebuild the site in many different ways, but the most common way is to run <code class="language-plaintext highlighter-rouge">jekyll serve</code>, which launches a web server and auto-regenerates your site when a file is updated.</p>

<p>Jekyll requires blog post files to be named according to the following format:</p>

<p><code class="language-plaintext highlighter-rouge">YEAR-MONTH-DAY-title.MARKUP</code></p>

<p>Where <code class="language-plaintext highlighter-rouge">YEAR</code> is a four-digit number, <code class="language-plaintext highlighter-rouge">MONTH</code> and <code class="language-plaintext highlighter-rouge">DAY</code> are both two-digit numbers, and <code class="language-plaintext highlighter-rouge">MARKUP</code> is the file extension representing the format used in the file. After that, include the necessary front matter. Take a look at the source for this post to get an idea about how it works.</p>

<p>Jekyll also offers powerful support for code snippets:</p>

<figure class="highlight"><pre><code class="language-ruby" data-lang="ruby"><span class="k">def</span> <span class="nf">print_hi</span><span class="p">(</span><span class="nb">name</span><span class="p">)</span>
  <span class="nb">puts</span> <span class="s2">"Hi, </span><span class="si">#{</span><span class="nb">name</span><span class="si">}</span><span class="s2">"</span>
<span class="k">end</span>
<span class="n">print_hi</span><span class="p">(</span><span class="s1">'Tom'</span><span class="p">)</span>
<span class="c1">#=&gt; prints 'Hi, Tom' to STDOUT.</span></code></pre></figure>

<p>Check out the <a href="https://jekyllrb.com/docs/home">Jekyll docs</a> for more info on how to get the most out of Jekyll. File all bugs/feature requests at <a href="https://github.com/jekyll/jekyll">Jekyll’s GitHub repo</a>. If you have questions, you can ask them on <a href="https://talk.jekyllrb.com/">Jekyll Talk</a>.</p>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[\[\sum_{n=1}^\infty 1/n^2 = \frac{\pi^2}{6}\]]]></summary></entry></feed>